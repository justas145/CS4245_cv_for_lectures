{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 326.4ms\n",
      "Speed: 11.0ms preprocess, 326.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5360778570175171, 0.7321938872337341)\n",
      "Center of normalized bounding box: (0.2217859923839569, 0.8370863199234009)\n",
      "Center of normalized bounding box: (0.17289859056472778, 0.830807626247406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 349.5ms\n",
      "Speed: 7.8ms preprocess, 349.5ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5424953103065491, 0.7292889952659607)\n",
      "Center of normalized bounding box: (0.22159266471862793, 0.8358849883079529)\n",
      "Center of normalized bounding box: (0.17126980423927307, 0.8309844732284546)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 503.4ms\n",
      "Speed: 12.0ms preprocess, 503.4ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5237771272659302, 0.7216236591339111)\n",
      "Center of normalized bounding box: (0.157235786318779, 0.8344599008560181)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 384.2ms\n",
      "Speed: 8.0ms preprocess, 384.2ms inference, 11.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5252957940101624, 0.7227658629417419)\n",
      "Center of normalized bounding box: (0.1735636293888092, 0.8291013240814209)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 425.2ms\n",
      "Speed: 7.2ms preprocess, 425.2ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5673285126686096, 0.7369344830513)\n",
      "Center of normalized bounding box: (0.17588001489639282, 0.8281809091567993)\n",
      "Center of normalized bounding box: (0.22787195444107056, 0.840213418006897)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 322.5ms\n",
      "Speed: 5.9ms preprocess, 322.5ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5663582682609558, 0.7363612651824951)\n",
      "Center of normalized bounding box: (0.1778651773929596, 0.8273425102233887)\n",
      "Center of normalized bounding box: (0.22789007425308228, 0.8417078852653503)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 334.4ms\n",
      "Speed: 13.0ms preprocess, 334.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5693483948707581, 0.7354605197906494)\n",
      "Center of normalized bounding box: (0.17837518453598022, 0.827330470085144)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 388.1ms\n",
      "Speed: 30.7ms preprocess, 388.1ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5658752918243408, 0.7349737286567688)\n",
      "Center of normalized bounding box: (0.17799191176891327, 0.8288102149963379)\n",
      "Center of normalized bounding box: (0.22674116492271423, 0.8443745970726013)\n",
      "Center of normalized bounding box: (0.133343905210495, 0.8377251625061035)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 382.3ms\n",
      "Speed: 9.5ms preprocess, 382.3ms inference, 10.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5705828070640564, 0.7350682616233826)\n",
      "Center of normalized bounding box: (0.17787566781044006, 0.8277236223220825)\n",
      "Center of normalized bounding box: (0.22757139801979065, 0.837409496307373)\n",
      "Center of normalized bounding box: (0.13660390675067902, 0.8376059532165527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 441.2ms\n",
      "Speed: 6.0ms preprocess, 441.2ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5673218965530396, 0.7355939745903015)\n",
      "Center of normalized bounding box: (0.17802542448043823, 0.8282356858253479)\n",
      "Center of normalized bounding box: (0.22590374946594238, 0.8397913575172424)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 463.5ms\n",
      "Speed: 13.0ms preprocess, 463.5ms inference, 8.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5665633678436279, 0.736320436000824)\n",
      "Center of normalized bounding box: (0.17800533771514893, 0.8280468583106995)\n",
      "Center of normalized bounding box: (0.1356368362903595, 0.8376725912094116)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 368.2ms\n",
      "Speed: 8.0ms preprocess, 368.2ms inference, 10.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5674734115600586, 0.7361956238746643)\n",
      "Center of normalized bounding box: (0.17513707280158997, 0.829246461391449)\n",
      "Center of normalized bounding box: (0.22621044516563416, 0.8407735824584961)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 636.9ms\n",
      "Speed: 3.0ms preprocess, 636.9ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5647466778755188, 0.7360133528709412)\n",
      "Center of normalized bounding box: (0.17260918021202087, 0.8295528888702393)\n",
      "Center of normalized bounding box: (0.22337493300437927, 0.8386784195899963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 943.3ms\n",
      "Speed: 11.7ms preprocess, 943.3ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.544109582901001, 0.7359612584114075)\n",
      "Center of normalized bounding box: (0.1724187135696411, 0.8298091888427734)\n",
      "Center of normalized bounding box: (0.2262355387210846, 0.8389446139335632)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 771.4ms\n",
      "Speed: 26.0ms preprocess, 771.4ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5676165223121643, 0.7366464138031006)\n",
      "Center of normalized bounding box: (0.17270731925964355, 0.830579400062561)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 618.6ms\n",
      "Speed: 14.3ms preprocess, 618.6ms inference, 7.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5661455988883972, 0.736853837966919)\n",
      "Center of normalized bounding box: (0.17011132836341858, 0.8303953409194946)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 946.8ms\n",
      "Speed: 22.0ms preprocess, 946.8ms inference, 9.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5654295682907104, 0.7367204427719116)\n",
      "Center of normalized bounding box: (0.17307068407535553, 0.8297601938247681)\n",
      "Center of normalized bounding box: (0.22109535336494446, 0.8429960012435913)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 821.4ms\n",
      "Speed: 15.1ms preprocess, 821.4ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5652741193771362, 0.7370887398719788)\n",
      "Center of normalized bounding box: (0.2230343371629715, 0.8472446203231812)\n",
      "Center of normalized bounding box: (0.15632827579975128, 0.8339624404907227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 458.0ms\n",
      "Speed: 9.4ms preprocess, 458.0ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5696161389350891, 0.736798882484436)\n",
      "Center of normalized bounding box: (0.17228993773460388, 0.8320451974868774)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 457.0ms\n",
      "Speed: 3.7ms preprocess, 457.0ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5656223893165588, 0.7370636463165283)\n",
      "Center of normalized bounding box: (0.19167402386665344, 0.8260172605514526)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 448.8ms\n",
      "Speed: 4.2ms preprocess, 448.8ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5668206810951233, 0.7369505763053894)\n",
      "Center of normalized bounding box: (0.196102112531662, 0.8296669721603394)\n",
      "Center of normalized bounding box: (0.14321240782737732, 0.8361703157424927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 435.4ms\n",
      "Speed: 6.5ms preprocess, 435.4ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5656573176383972, 0.7372311353683472)\n",
      "Center of normalized bounding box: (0.19338563084602356, 0.8290562629699707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 392.1ms\n",
      "Speed: 5.6ms preprocess, 392.1ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5667195916175842, 0.7375537157058716)\n",
      "Center of normalized bounding box: (0.19066089391708374, 0.8289313316345215)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 385.0ms\n",
      "Speed: 0.0ms preprocess, 385.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5417618155479431, 0.7372248768806458)\n",
      "Center of normalized bounding box: (0.19052310287952423, 0.82970130443573)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 356.1ms\n",
      "Speed: 3.2ms preprocess, 356.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.568325400352478, 0.736102283000946)\n",
      "Center of normalized bounding box: (0.1876779943704605, 0.8290970325469971)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 362.6ms\n",
      "Speed: 5.5ms preprocess, 362.6ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5698206424713135, 0.7368766665458679)\n",
      "Center of normalized bounding box: (0.18785467743873596, 0.8287257552146912)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 367.7ms\n",
      "Speed: 7.5ms preprocess, 367.7ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5669987797737122, 0.7368969917297363)\n",
      "Center of normalized bounding box: (0.18565109372138977, 0.8289440274238586)\n",
      "Center of normalized bounding box: (0.142045259475708, 0.8344194889068604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 368.1ms\n",
      "Speed: 4.5ms preprocess, 368.1ms inference, 9.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5687963962554932, 0.7367507219314575)\n",
      "Center of normalized bounding box: (0.18562451004981995, 0.8284804224967957)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 395.8ms\n",
      "Speed: 8.0ms preprocess, 395.8ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.568416953086853, 0.7366381287574768)\n",
      "Center of normalized bounding box: (0.18365339934825897, 0.8291926383972168)\n",
      "Center of normalized bounding box: (0.1377066671848297, 0.8358813524246216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 422.2ms\n",
      "Speed: 2.5ms preprocess, 422.2ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.570300281047821, 0.7369384169578552)\n",
      "Center of normalized bounding box: (0.18709436058998108, 0.8301132917404175)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 543.4ms\n",
      "Speed: 13.0ms preprocess, 543.4ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5664530396461487, 0.7377122044563293)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 533.1ms\n",
      "Speed: 11.4ms preprocess, 533.1ms inference, 8.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5690582990646362, 0.7378720641136169)\n",
      "Center of normalized bounding box: (0.18389223515987396, 0.8282831907272339)\n",
      "Center of normalized bounding box: (0.13403192162513733, 0.840518593788147)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 510.5ms\n",
      "Speed: 9.6ms preprocess, 510.5ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5685957074165344, 0.7371325492858887)\n",
      "Center of normalized bounding box: (0.18590623140335083, 0.8282067775726318)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 542.1ms\n",
      "Speed: 7.8ms preprocess, 542.1ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5718144774436951, 0.7370095252990723)\n",
      "Center of normalized bounding box: (0.2133205533027649, 0.8321536779403687)\n",
      "Center of normalized bounding box: (0.1338600218296051, 0.8394225835800171)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 527.0ms\n",
      "Speed: 9.6ms preprocess, 527.0ms inference, 8.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5671382546424866, 0.7368605136871338)\n",
      "Center of normalized bounding box: (0.20921802520751953, 0.8306471109390259)\n",
      "Center of normalized bounding box: (0.2179296910762787, 0.8339864015579224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 585.8ms\n",
      "Speed: 11.5ms preprocess, 585.8ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5681472420692444, 0.7376520037651062)\n",
      "Center of normalized bounding box: (0.2173864245414734, 0.8363550901412964)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 586.3ms\n",
      "Speed: 11.0ms preprocess, 586.3ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5668897032737732, 0.7375713586807251)\n",
      "Center of normalized bounding box: (0.21635422110557556, 0.8340702652931213)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 470.3ms\n",
      "Speed: 9.8ms preprocess, 470.3ms inference, 7.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5640883445739746, 0.7373242378234863)\n",
      "Center of normalized bounding box: (0.20906595885753632, 0.8312175869941711)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 433.5ms\n",
      "Speed: 7.9ms preprocess, 433.5ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.567510724067688, 0.7373231649398804)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 485.3ms\n",
      "Speed: 8.9ms preprocess, 485.3ms inference, 8.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5647411942481995, 0.7371766567230225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 491.6ms\n",
      "Speed: 8.3ms preprocess, 491.6ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5696747899055481, 0.7371734976768494)\n",
      "Center of normalized bounding box: (0.17978796362876892, 0.8333238363265991)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 537.1ms\n",
      "Speed: 9.0ms preprocess, 537.1ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5660497546195984, 0.7371918559074402)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 529.3ms\n",
      "Speed: 10.1ms preprocess, 529.3ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5677798986434937, 0.7374706268310547)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 561.2ms\n",
      "Speed: 9.4ms preprocess, 561.2ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5666421055793762, 0.7376936078071594)\n",
      "Center of normalized bounding box: (0.18173058331012726, 0.83262038230896)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 538.6ms\n",
      "Speed: 8.1ms preprocess, 538.6ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5652786493301392, 0.7374581694602966)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 577.6ms\n",
      "Speed: 12.2ms preprocess, 577.6ms inference, 9.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5681770443916321, 0.7374120354652405)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 504.2ms\n",
      "Speed: 10.0ms preprocess, 504.2ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5668822526931763, 0.7375655174255371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 527.4ms\n",
      "Speed: 9.9ms preprocess, 527.4ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5679925084114075, 0.7375361919403076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 529.6ms\n",
      "Speed: 9.4ms preprocess, 529.6ms inference, 7.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5668584704399109, 0.7371343374252319)\n",
      "Center of normalized bounding box: (0.19400402903556824, 0.8294916749000549)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 508.9ms\n",
      "Speed: 8.6ms preprocess, 508.9ms inference, 7.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5661932826042175, 0.7373197078704834)\n",
      "Center of normalized bounding box: (0.20660343766212463, 0.8309280276298523)\n",
      "Center of normalized bounding box: (0.21700593829154968, 0.8354940414428711)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 594.7ms\n",
      "Speed: 9.4ms preprocess, 594.7ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5641804337501526, 0.737318217754364)\n",
      "Center of normalized bounding box: (0.21676519513130188, 0.83412766456604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 544.9ms\n",
      "Speed: 9.5ms preprocess, 544.9ms inference, 8.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5696839690208435, 0.7370213270187378)\n",
      "Center of normalized bounding box: (0.1855258047580719, 0.8296424150466919)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 564.9ms\n",
      "Speed: 11.5ms preprocess, 564.9ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5672882199287415, 0.7370454668998718)\n",
      "Center of normalized bounding box: (0.179388165473938, 0.8306639790534973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 587.4ms\n",
      "Speed: 10.8ms preprocess, 587.4ms inference, 10.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5664345026016235, 0.7368344664573669)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 509.2ms\n",
      "Speed: 9.7ms preprocess, 509.2ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5659884214401245, 0.737139105796814)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 517.2ms\n",
      "Speed: 10.3ms preprocess, 517.2ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5641051530838013, 0.7375943064689636)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 512.2ms\n",
      "Speed: 10.8ms preprocess, 512.2ms inference, 9.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5671913027763367, 0.7377312183380127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 529.9ms\n",
      "Speed: 9.3ms preprocess, 529.9ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5658367872238159, 0.7373982667922974)\n",
      "Center of normalized bounding box: (0.1801815629005432, 0.8303544521331787)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 546.1ms\n",
      "Speed: 9.7ms preprocess, 546.1ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5669364929199219, 0.7372506856918335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 495.6ms\n",
      "Speed: 7.6ms preprocess, 495.6ms inference, 7.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5649142861366272, 0.7374178171157837)\n",
      "Center of normalized bounding box: (0.17649126052856445, 0.832171618938446)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 538.5ms\n",
      "Speed: 10.8ms preprocess, 538.5ms inference, 8.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5649175643920898, 0.7372886538505554)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 584.8ms\n",
      "Speed: 8.5ms preprocess, 584.8ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5659401416778564, 0.7372941374778748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 548.8ms\n",
      "Speed: 9.4ms preprocess, 548.8ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5362200736999512, 0.7370501160621643)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 510.0ms\n",
      "Speed: 8.8ms preprocess, 510.0ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5669563412666321, 0.7373356223106384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 531.2ms\n",
      "Speed: 9.8ms preprocess, 531.2ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5669677257537842, 0.7373464703559875)\n",
      "Center of normalized bounding box: (0.1751018762588501, 0.8308559656143188)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 534.7ms\n",
      "Speed: 11.1ms preprocess, 534.7ms inference, 7.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.569307267665863, 0.7375261783599854)\n",
      "Center of normalized bounding box: (0.17320066690444946, 0.8308055996894836)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 568.2ms\n",
      "Speed: 10.8ms preprocess, 568.2ms inference, 10.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5639495849609375, 0.7373461723327637)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 547.9ms\n",
      "Speed: 12.9ms preprocess, 547.9ms inference, 9.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5651107430458069, 0.7374459505081177)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 569.1ms\n",
      "Speed: 8.7ms preprocess, 569.1ms inference, 9.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.568341851234436, 0.7375472187995911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 539.9ms\n",
      "Speed: 11.8ms preprocess, 539.9ms inference, 10.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.564934253692627, 0.7372344732284546)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 561.3ms\n",
      "Speed: 11.1ms preprocess, 561.3ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5421499013900757, 0.7366251945495605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 536.8ms\n",
      "Speed: 7.1ms preprocess, 536.8ms inference, 11.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5641233325004578, 0.7372699975967407)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 597.0ms\n",
      "Speed: 10.9ms preprocess, 597.0ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5685946941375732, 0.7374032139778137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 555.1ms\n",
      "Speed: 13.0ms preprocess, 555.1ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5640407800674438, 0.7373362183570862)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 500.8ms\n",
      "Speed: 10.8ms preprocess, 500.8ms inference, 8.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5670191645622253, 0.7373576164245605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 532.9ms\n",
      "Speed: 10.3ms preprocess, 532.9ms inference, 9.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5672570466995239, 0.737267017364502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 526.0ms\n",
      "Speed: 11.0ms preprocess, 526.0ms inference, 10.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5685189366340637, 0.7373670339584351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 557.6ms\n",
      "Speed: 9.3ms preprocess, 557.6ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5654739141464233, 0.7372922897338867)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 596.4ms\n",
      "Speed: 11.0ms preprocess, 596.4ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5644032955169678, 0.7372406125068665)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 547.0ms\n",
      "Speed: 11.7ms preprocess, 547.0ms inference, 12.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5702900886535645, 0.7381696105003357)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 593.3ms\n",
      "Speed: 10.6ms preprocess, 593.3ms inference, 9.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5654222965240479, 0.7390180826187134)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 531.1ms\n",
      "Speed: 11.4ms preprocess, 531.1ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5617395639419556, 0.7316606044769287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 570.0ms\n",
      "Speed: 8.2ms preprocess, 570.0ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.48315757513046265, 0.7041271924972534)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 1 airplane, 560.7ms\n",
      "Speed: 12.0ms preprocess, 560.7ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.4373760223388672, 0.7017927169799805)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 517.9ms\n",
      "Speed: 10.1ms preprocess, 517.9ms inference, 7.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.49171823263168335, 0.6951815485954285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 1 chair, 559.9ms\n",
      "Speed: 11.9ms preprocess, 559.9ms inference, 9.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5029441118240356, 0.6879335641860962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 573.8ms\n",
      "Speed: 9.1ms preprocess, 573.8ms inference, 9.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.5120825171470642, 0.650371789932251)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 608.4ms\n",
      "Speed: 9.0ms preprocess, 608.4ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.48032450675964355, 0.6501508355140686)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 722.5ms\n",
      "Speed: 10.5ms preprocess, 722.5ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.45384645462036133, 0.6536726951599121)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 565.5ms\n",
      "Speed: 12.1ms preprocess, 565.5ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.473021537065506, 0.6478589773178101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 490.2ms\n",
      "Speed: 3.6ms preprocess, 490.2ms inference, 9.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.44207820296287537, 0.6474956274032593)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 586.9ms\n",
      "Speed: 10.8ms preprocess, 586.9ms inference, 10.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.45573052763938904, 0.6466924548149109)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 759.0ms\n",
      "Speed: 9.3ms preprocess, 759.0ms inference, 14.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.45524492859840393, 0.6471970081329346)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 587.6ms\n",
      "Speed: 13.5ms preprocess, 587.6ms inference, 11.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.45454245805740356, 0.6411430239677429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 700.3ms\n",
      "Speed: 13.0ms preprocess, 700.3ms inference, 13.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.4534423351287842, 0.6423856616020203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 620.0ms\n",
      "Speed: 8.0ms preprocess, 620.0ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.4566101133823395, 0.6456347703933716)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 609.2ms\n",
      "Speed: 10.8ms preprocess, 609.2ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.4766843318939209, 0.6449905633926392)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 656.6ms\n",
      "Speed: 11.0ms preprocess, 656.6ms inference, 9.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.47806161642074585, 0.6471827626228333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 707.0ms\n",
      "Speed: 11.2ms preprocess, 707.0ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.47619372606277466, 0.6473353505134583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 662.2ms\n",
      "Speed: 12.1ms preprocess, 662.2ms inference, 12.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of normalized bounding box: (0.4773913323879242, 0.6450772285461426)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# load model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Start capturing the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:  # You want continuous detection\n",
    "    # Read the current frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply the model to the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Process the results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes)):\n",
    "            if boxes.cls[i].item() == 0:  # 'person'\n",
    "                # Get normalized bounding box coordinates\n",
    "                x1, y1, x2, y2 = boxes.xyxyn[i]\n",
    "\n",
    "                # Calculate the normalized center of the bounding box\n",
    "                center_x = (x1 + x2) / 2\n",
    "                center_y = (y1 + y2) / 2\n",
    "\n",
    "                print(f\"Center of normalized bounding box {i}: ({center_x}, {center_y})\")\n",
    "\n",
    "    # Display the frame\n",
    "    # Visualize the results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "\n",
      "    WARNING  stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,\n",
      "    causing potential out-of-memory errors for large sources or long-running streams/videos.\n",
      "\n",
      "    Usage:\n",
      "        results = model(source=..., stream=True)  # generator of Results objects\n",
      "        for r in results:\n",
      "            boxes = r.boxes  # Boxes object for bbox outputs\n",
      "            masks = r.masks  # Masks object for segment masks outputs\n",
      "            probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 1 person, 215.2ms\n",
      "0: 480x640 1 person, 1 chair, 246.7ms\n",
      "0: 480x640 1 person, 1 chair, 217.2ms\n",
      "0: 480x640 1 person, 2 chairs, 195.2ms\n",
      "0: 480x640 1 person, 195.5ms\n",
      "0: 480x640 1 person, 1 chair, 213.7ms\n",
      "0: 480x640 1 person, 1 chair, 198.6ms\n",
      "0: 480x640 1 person, 1 chair, 196.1ms\n",
      "0: 480x640 1 person, 1 chair, 228.3ms\n",
      "0: 480x640 1 person, 1 chair, 323.6ms\n",
      "0: 480x640 1 person, 2 chairs, 309.2ms\n",
      "0: 480x640 1 person, 1 chair, 287.5ms\n",
      "0: 480x640 1 person, 1 chair, 257.5ms\n",
      "0: 480x640 1 person, 1 chair, 205.9ms\n",
      "0: 480x640 1 person, 173.2ms\n",
      "0: 480x640 1 person, 1 chair, 183.5ms\n",
      "0: 480x640 1 person, 1 chair, 192.9ms\n",
      "0: 480x640 1 person, 1 chair, 190.5ms\n",
      "0: 480x640 1 person, 198.2ms\n",
      "0: 480x640 1 person, 1 chair, 226.7ms\n",
      "0: 480x640 1 person, 2 chairs, 183.2ms\n",
      "0: 480x640 1 person, 2 chairs, 200.5ms\n",
      "0: 480x640 1 person, 1 chair, 189.9ms\n",
      "0: 480x640 1 person, 186.7ms\n",
      "0: 480x640 1 person, 1 chair, 181.8ms\n",
      "0: 480x640 1 person, 192.1ms\n",
      "0: 480x640 1 person, 1 chair, 195.1ms\n",
      "0: 480x640 1 person, 178.9ms\n",
      "0: 480x640 1 person, 1 chair, 183.9ms\n",
      "0: 480x640 1 person, 2 chairs, 195.8ms\n",
      "0: 480x640 1 person, 1 chair, 187.3ms\n",
      "0: 480x640 1 person, 1 chair, 212.1ms\n",
      "0: 480x640 1 person, 1 chair, 205.3ms\n",
      "0: 480x640 1 person, 227.1ms\n",
      "0: 480x640 1 person, 1 chair, 223.1ms\n",
      "0: 480x640 1 person, 2 chairs, 214.4ms\n",
      "0: 480x640 1 person, 1 chair, 177.5ms\n",
      "0: 480x640 1 person, 1 chair, 173.6ms\n",
      "0: 480x640 1 person, 2 chairs, 187.5ms\n",
      "0: 480x640 1 person, 1 airplane, 1 chair, 185.1ms\n",
      "0: 480x640 1 person, 1 chair, 178.3ms\n",
      "0: 480x640 1 person, 1 chair, 202.3ms\n",
      "0: 480x640 1 person, 1 chair, 201.5ms\n",
      "0: 480x640 1 person, 1 chair, 183.3ms\n",
      "0: 480x640 1 person, 1 chair, 181.9ms\n",
      "0: 480x640 1 person, 1 chair, 201.8ms\n",
      "0: 480x640 1 person, 1 chair, 184.1ms\n",
      "0: 480x640 1 person, 1 chair, 205.9ms\n",
      "0: 480x640 1 person, 1 chair, 178.7ms\n",
      "0: 480x640 1 person, 1 chair, 190.4ms\n",
      "0: 480x640 1 person, 1 chair, 194.4ms\n",
      "0: 480x640 1 person, 1 chair, 174.6ms\n",
      "0: 480x640 1 person, 1 airplane, 208.0ms\n",
      "0: 480x640 1 person, 1 chair, 215.2ms\n",
      "0: 480x640 1 person, 3 chairs, 255.7ms\n",
      "0: 480x640 1 person, 275.7ms\n",
      "0: 480x640 1 person, 276.0ms\n",
      "0: 480x640 1 person, 1 chair, 231.1ms\n",
      "0: 480x640 1 person, 1 chair, 226.4ms\n",
      "0: 480x640 1 person, 223.0ms\n",
      "0: 480x640 1 person, 194.7ms\n",
      "0: 480x640 1 person, 176.4ms\n",
      "0: 480x640 1 person, 197.2ms\n",
      "0: 480x640 1 person, 173.8ms\n",
      "0: 480x640 1 person, 208.1ms\n",
      "0: 480x640 1 person, 189.1ms\n",
      "0: 480x640 1 person, 189.8ms\n",
      "0: 480x640 1 person, 1 donut, 213.1ms\n",
      "0: 480x640 2 persons, 209.1ms\n",
      "0: 480x640 1 person, 216.2ms\n",
      "0: 480x640 2 persons, 1 donut, 231.9ms\n",
      "0: 480x640 1 person, 220.5ms\n",
      "0: 480x640 1 person, 1 donut, 208.1ms\n",
      "0: 480x640 2 persons, 181.1ms\n",
      "0: 480x640 1 person, 188.2ms\n",
      "0: 480x640 1 person, 1 donut, 191.0ms\n",
      "0: 480x640 1 person, 202.7ms\n",
      "0: 480x640 1 person, 232.5ms\n",
      "0: 480x640 1 person, 231.9ms\n",
      "0: 480x640 2 persons, 204.9ms\n",
      "0: 480x640 2 persons, 203.3ms\n",
      "0: 480x640 1 person, 210.4ms\n",
      "0: 480x640 1 person, 198.3ms\n",
      "0: 480x640 2 persons, 215.1ms\n",
      "0: 480x640 1 person, 207.3ms\n",
      "0: 480x640 1 person, 218.2ms\n",
      "0: 480x640 1 person, 201.0ms\n",
      "0: 480x640 1 person, 199.2ms\n",
      "0: 480x640 1 person, 199.2ms\n",
      "0: 480x640 1 person, 205.8ms\n",
      "0: 480x640 1 person, 210.8ms\n",
      "0: 480x640 1 person, 224.1ms\n",
      "0: 480x640 1 person, 244.2ms\n",
      "0: 480x640 1 person, 239.6ms\n",
      "0: 480x640 1 person, 198.3ms\n",
      "0: 480x640 1 person, 189.2ms\n",
      "0: 480x640 1 person, 205.7ms\n",
      "0: 480x640 1 person, 190.3ms\n",
      "0: 480x640 1 person, 188.0ms\n",
      "0: 480x640 1 person, 210.9ms\n",
      "0: 480x640 1 person, 220.3ms\n",
      "0: 480x640 1 person, 202.2ms\n",
      "0: 480x640 1 person, 210.2ms\n",
      "0: 480x640 1 person, 197.7ms\n",
      "0: 480x640 1 person, 187.1ms\n",
      "0: 480x640 1 person, 198.0ms\n",
      "0: 480x640 1 person, 200.6ms\n",
      "0: 480x640 1 person, 225.4ms\n",
      "0: 480x640 1 person, 187.9ms\n",
      "0: 480x640 2 persons, 210.6ms\n",
      "0: 480x640 1 person, 194.9ms\n",
      "0: 480x640 1 person, 270.6ms\n",
      "0: 480x640 1 person, 220.4ms\n",
      "0: 480x640 2 persons, 224.0ms\n",
      "0: 480x640 1 person, 243.6ms\n",
      "0: 480x640 1 person, 232.8ms\n",
      "0: 480x640 1 person, 214.4ms\n",
      "0: 480x640 1 person, 187.8ms\n",
      "0: 480x640 2 persons, 182.4ms\n",
      "0: 480x640 1 person, 187.9ms\n",
      "0: 480x640 1 person, 194.7ms\n",
      "0: 480x640 1 person, 198.2ms\n",
      "0: 480x640 1 person, 227.0ms\n",
      "0: 480x640 1 person, 203.2ms\n",
      "0: 480x640 1 person, 186.1ms\n",
      "0: 480x640 1 person, 206.4ms\n",
      "0: 480x640 1 person, 189.4ms\n",
      "0: 480x640 1 person, 205.7ms\n",
      "0: 480x640 1 person, 191.3ms\n",
      "0: 480x640 1 person, 193.9ms\n",
      "0: 480x640 2 persons, 196.2ms\n",
      "0: 480x640 1 person, 192.4ms\n",
      "0: 480x640 2 persons, 201.0ms\n",
      "0: 480x640 2 persons, 199.0ms\n",
      "0: 480x640 2 persons, 219.5ms\n",
      "0: 480x640 2 persons, 216.4ms\n",
      "0: 480x640 1 person, 229.8ms\n",
      "0: 480x640 1 person, 235.9ms\n",
      "0: 480x640 1 person, 238.0ms\n",
      "0: 480x640 1 person, 183.1ms\n",
      "0: 480x640 1 person, 203.8ms\n",
      "0: 480x640 1 person, 188.4ms\n",
      "0: 480x640 1 person, 189.2ms\n",
      "0: 480x640 1 person, 191.6ms\n",
      "0: 480x640 1 person, 220.2ms\n",
      "0: 480x640 1 person, 201.2ms\n",
      "0: 480x640 1 person, 200.6ms\n",
      "0: 480x640 1 person, 190.3ms\n",
      "0: 480x640 1 person, 196.5ms\n",
      "0: 480x640 1 person, 188.1ms\n",
      "0: 480x640 1 person, 199.1ms\n",
      "0: 480x640 1 person, 185.1ms\n",
      "0: 480x640 1 person, 197.7ms\n",
      "0: 480x640 1 person, 184.6ms\n",
      "0: 480x640 1 person, 185.8ms\n",
      "0: 480x640 1 person, 189.7ms\n",
      "0: 480x640 1 person, 203.2ms\n",
      "0: 480x640 1 person, 218.2ms\n",
      "0: 480x640 1 person, 221.0ms\n",
      "0: 480x640 1 person, 235.2ms\n",
      "0: 480x640 2 persons, 227.6ms\n",
      "0: 480x640 1 person, 195.9ms\n",
      "0: 480x640 1 person, 180.5ms\n",
      "0: 480x640 1 person, 192.0ms\n",
      "0: 480x640 1 person, 209.2ms\n",
      "0: 480x640 1 person, 186.4ms\n",
      "0: 480x640 1 person, 198.9ms\n",
      "0: 480x640 2 persons, 218.3ms\n",
      "0: 480x640 1 person, 204.5ms\n",
      "0: 480x640 2 persons, 187.1ms\n",
      "0: 480x640 2 persons, 201.2ms\n",
      "0: 480x640 2 persons, 183.4ms\n",
      "0: 480x640 2 persons, 184.4ms\n",
      "0: 480x640 2 persons, 187.1ms\n",
      "0: 480x640 2 persons, 210.4ms\n",
      "0: 480x640 2 persons, 197.8ms\n",
      "0: 480x640 2 persons, 1 chair, 200.7ms\n",
      "0: 480x640 2 persons, 1 chair, 188.6ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39myolov8n.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m results \u001b[39m=\u001b[39m model(source\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, show\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:111\u001b[0m, in \u001b[0;36mYOLO.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(source, stream, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:253\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, overrides)\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model)\n\u001b[0;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model))\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m             \u001b[39m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 56\u001b[0m                 response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(request)\n\u001b[0;32m     58\u001b[0m \u001b[39m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[39m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:240\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> 240\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n\u001b[0;32m    242\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:314\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    311\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[0;32m    315\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:213\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[1;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[0;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_augment(x)  \u001b[39m# augmented inference, None\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_once(x, profile, visualize)\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:64\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[0;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m---> 64\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[0;32m     65\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\nn\\modules\\head.py:60\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     box, \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m x_cat\u001b[39m.\u001b[39msplit((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_max \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnc), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m dbox \u001b[39m=\u001b[39m dist2bbox(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdfl(box), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchors\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), xywh\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrides\n\u001b[0;32m     61\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((dbox, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msigmoid()), \u001b[39m1\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m y \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexport \u001b[39melse\u001b[39;00m (y, x)\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\.venv\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:35\u001b[0m, in \u001b[0;36mDFL.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Applies a transformer layer on input tensor 'x' and returns a tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m b, c, a \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape  \u001b[39m# batch, channels, anchors\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x\u001b[39m.\u001b[39;49mview(b, \u001b[39m4\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc1, a)\u001b[39m.\u001b[39;49mtranspose(\u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49msoftmax(\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mview(b, \u001b[39m4\u001b[39m, a)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "results = model(source=0, show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = results[0].boxes\n",
    "box = boxes[0]  # returns one box\n",
    "box.xyxyn\n",
    "boxes.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 147.2ms\n",
      "Speed: 4.0ms preprocess, 147.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000002244F0D0270>, array([[[100,  96, 113],\n        [100,  96, 114],\n        [100,  95, 116],\n        ...,\n        [ 74,  88, 126],\n        [ 75,  89, 124],\n        [ 76,  89, 122]],\n\n       [[102,  99, 115],\n        [ 97,  93, 113],\n        [106, 103, 126],\n        ...,\n        [ 77,  93, 132],\n        [ 78,  93, 131],\n        [ 78,  93, 128]],\n\n       [[103, 101, 119],\n        [ 92,  91, 112],\n        [126, 125, 150],\n        ...,\n        [ 75,  95, 136],\n        [ 76,  94, 134],\n        [ 78,  95, 132]],\n\n       ...,\n\n       [[185, 185, 185],\n        [185, 184, 184],\n        [186, 184, 184],\n        ...,\n        [160, 174, 191],\n        [161, 173, 186],\n        [166, 175, 186]],\n\n       [[164, 161, 164],\n        [152, 147, 149],\n        [134, 127, 128],\n        ...,\n        [151, 168, 194],\n        [154, 170, 192],\n        [155, 169, 188]],\n\n       [[114, 108, 108],\n        [108, 100,  98],\n        [106,  95,  92],\n        ...,\n        [ 93, 113, 146],\n        [ 91, 110, 140],\n        [ 94, 113, 141]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000002244F154670>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\playgroud_notebook.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     face_image \u001b[39m=\u001b[39m person_image[top:bottom, left:right]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# Compute the face encoding of the face\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     face_encodings \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39;49mface_encodings(face_image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# If this is the first person we've seen, save their face encoding\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m first_person_face_encoding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m face_encodings:\n",
      "File \u001b[1;32mc:\\Users\\justa\\anaconda3\\envs\\cv_project\\lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[39m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m [np\u001b[39m.\u001b[39marray(face_encoder\u001b[39m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[39mfor\u001b[39;00m raw_landmark_set \u001b[39min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[1;32mc:\\Users\\justa\\anaconda3\\envs\\cv_project\\lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[39m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m [np\u001b[39m.\u001b[39marray(face_encoder\u001b[39m.\u001b[39;49mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[39mfor\u001b[39;00m raw_landmark_set \u001b[39min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000002244F0D0270>, array([[[100,  96, 113],\n        [100,  96, 114],\n        [100,  95, 116],\n        ...,\n        [ 74,  88, 126],\n        [ 75,  89, 124],\n        [ 76,  89, 122]],\n\n       [[102,  99, 115],\n        [ 97,  93, 113],\n        [106, 103, 126],\n        ...,\n        [ 77,  93, 132],\n        [ 78,  93, 131],\n        [ 78,  93, 128]],\n\n       [[103, 101, 119],\n        [ 92,  91, 112],\n        [126, 125, 150],\n        ...,\n        [ 75,  95, 136],\n        [ 76,  94, 134],\n        [ 78,  95, 132]],\n\n       ...,\n\n       [[185, 185, 185],\n        [185, 184, 184],\n        [186, 184, 184],\n        ...,\n        [160, 174, 191],\n        [161, 173, 186],\n        [166, 175, 186]],\n\n       [[164, 161, 164],\n        [152, 147, 149],\n        [134, 127, 128],\n        ...,\n        [151, 168, 194],\n        [154, 170, 192],\n        [155, 169, 188]],\n\n       [[114, 108, 108],\n        [108, 100,  98],\n        [106,  95,  92],\n        ...,\n        [ 93, 113, 146],\n        [ 91, 110, 140],\n        [ 94, 113, 141]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000002244F154670>, 1"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# load model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# This will hold the face encoding of the first person we see\n",
    "first_person_face_encoding = None\n",
    "\n",
    "# Start capturing the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:  # You want continuous detection\n",
    "    # Read the current frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply the model to the frame\n",
    "    results = model(frame)\n",
    "    # Get frame dimensions\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    # Process the results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes)):\n",
    "            if boxes.cls[i].item() == 0:  # 'person'\n",
    "                # Get bounding box coordinates\n",
    "                x1_n, y1_n, x2_n, y2_n = boxes.xyxyn[i]\n",
    "                \n",
    "                # Convert coordinates to original scale\n",
    "                x1 = int(x1_n * frame_width)\n",
    "                x2 = int(x2_n * frame_width)\n",
    "                y1 = int(y1_n * frame_height)\n",
    "                y2 = int(y2_n * frame_height)\n",
    "\n",
    "                # Extract the person's image from the frame\n",
    "                person_image = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Now, use face_recognition to find faces in the person_image\n",
    "                face_locations = face_recognition.face_locations(person_image)\n",
    "                if face_locations:\n",
    "                    # If a face is detected, get the first face image.\n",
    "                    top, right, bottom, left = face_locations[0]\n",
    "                    face_image = person_image[top:bottom, left:right]\n",
    "\n",
    "                    # Compute the face encoding of the face\n",
    "                    face_encodings = face_recognition.face_encodings(face_image)\n",
    "\n",
    "\n",
    "                # If this is the first person we've seen, save their face encoding\n",
    "                if first_person_face_encoding is None and face_encodings:\n",
    "                    first_person_face_encoding = face_encodings[0]\n",
    "\n",
    "                # Otherwise, compare the face to the first person's face\n",
    "                elif face_encodings:\n",
    "                    match = face_recognition.compare_faces([first_person_face_encoding], face_encodings[0])\n",
    "                    if match[0]:\n",
    "                        # Draw a green circle on the frame\n",
    "                        cv2.circle(frame, (center_x, center_y), 5, (0, 255, 0), -1)\n",
    "                    else:\n",
    "                        # Draw a red circle on the frame\n",
    "                        cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"YOLOv8 Inference\", frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 178.2ms\n",
      "Speed: 5.0ms preprocess, 178.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detected!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000002244F0D0270>, array([[[189, 185, 185],\n        [190, 186, 186],\n        [189, 185, 185],\n        ...,\n        [164, 170, 183],\n        [166, 172, 185],\n        [166, 172, 186]],\n\n       [[183, 179, 179],\n        [189, 185, 185],\n        [188, 183, 184],\n        ...,\n        [167, 173, 187],\n        [167, 173, 188],\n        [166, 172, 187]],\n\n       [[174, 170, 170],\n        [189, 185, 185],\n        [189, 185, 185],\n        ...,\n        [167, 173, 187],\n        [167, 173, 187],\n        [167, 173, 187]],\n\n       ...,\n\n       [[105,  78,  47],\n        [105,  78,  46],\n        [104,  78,  46],\n        ...,\n        [ 98,  88,  71],\n        [ 99,  89,  72],\n        [ 96,  87,  69]],\n\n       [[105,  78,  47],\n        [104,  77,  45],\n        [104,  77,  45],\n        ...,\n        [100,  88,  71],\n        [100,  88,  71],\n        [ 97,  85,  68]],\n\n       [[103,  78,  47],\n        [103,  78,  46],\n        [102,  77,  45],\n        ...,\n        [ 98,  87,  69],\n        [ 98,  87,  69],\n        [ 97,  85,  68]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x0000022465236B30>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\playgroud_notebook.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mFace\u001b[39m\u001b[39m\"\u001b[39m, face_image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m                     \u001b[39m# Compute the face encoding of the face\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                     face_encodings \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39;49mface_encodings(face_image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W5sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mface encoded!!!\u001b[39m\u001b[39m'\u001b[39m, face_encodings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W5sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m                     \u001b[39m# Continue with the rest of your code...\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W5sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Break the loop if 'q' is pressed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\justa\\anaconda3\\envs\\cv_project\\lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[39m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m [np\u001b[39m.\u001b[39marray(face_encoder\u001b[39m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[39mfor\u001b[39;00m raw_landmark_set \u001b[39min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[1;32mc:\\Users\\justa\\anaconda3\\envs\\cv_project\\lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[39m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m [np\u001b[39m.\u001b[39marray(face_encoder\u001b[39m.\u001b[39;49mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[39mfor\u001b[39;00m raw_landmark_set \u001b[39min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000002244F0D0270>, array([[[189, 185, 185],\n        [190, 186, 186],\n        [189, 185, 185],\n        ...,\n        [164, 170, 183],\n        [166, 172, 185],\n        [166, 172, 186]],\n\n       [[183, 179, 179],\n        [189, 185, 185],\n        [188, 183, 184],\n        ...,\n        [167, 173, 187],\n        [167, 173, 188],\n        [166, 172, 187]],\n\n       [[174, 170, 170],\n        [189, 185, 185],\n        [189, 185, 185],\n        ...,\n        [167, 173, 187],\n        [167, 173, 187],\n        [167, 173, 187]],\n\n       ...,\n\n       [[105,  78,  47],\n        [105,  78,  46],\n        [104,  78,  46],\n        ...,\n        [ 98,  88,  71],\n        [ 99,  89,  72],\n        [ 96,  87,  69]],\n\n       [[105,  78,  47],\n        [104,  77,  45],\n        [104,  77,  45],\n        ...,\n        [100,  88,  71],\n        [100,  88,  71],\n        [ 97,  85,  68]],\n\n       [[103,  78,  47],\n        [103,  78,  46],\n        [102,  77,  45],\n        ...,\n        [ 98,  87,  69],\n        [ 98,  87,  69],\n        [ 97,  85,  68]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x0000022465236B30>, 1"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# load model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# This will hold the face encoding of the first person we see\n",
    "first_person_face_encoding = None\n",
    "\n",
    "# Start capturing the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:  # You want continuous detection\n",
    "    # Read the current frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply the model to the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "\n",
    "    # Process the results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes)):\n",
    "            if boxes.cls[i].item() == 0:  # 'person'\n",
    "                # Get normalized bounding box coordinates\n",
    "                x1, y1, x2, y2 = boxes.xyxyn[i]\n",
    "\n",
    "                # Convert coordinates to original scale\n",
    "                x1 = int(x1 * frame_width)\n",
    "                x2 = int(x2 * frame_width)\n",
    "                y1 = int(y1 * frame_height)\n",
    "                y2 = int(y2 * frame_height)\n",
    "\n",
    "                # Extract the person's image from the frame\n",
    "                person_image = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Now, use face_recognition to find faces in the person_image\n",
    "                face_locations = face_recognition.face_locations(person_image)\n",
    "                if face_locations:\n",
    "                    print(\"Face detected!\")\n",
    "                    # If a face is detected, get the first face image.\n",
    "                    top, right, bottom, left = face_locations[0]\n",
    "                    padding = 20  # adjust as needed\n",
    "                    top = max(0, top - padding)\n",
    "                    right = min(person_image.shape[1], right + padding)\n",
    "                    bottom = min(person_image.shape[0], bottom + padding)\n",
    "                    left = max(0, left - padding)\n",
    "                    face_image = person_image[top:bottom, left:right]\n",
    "\n",
    "                    # Check if face_image is not empty\n",
    "                    if face_image.size != 0 and face_image.shape[0] > 0 and face_image.shape[1] > 0:\n",
    "                        # Display the face_image in a new window\n",
    "                        cv2.imshow(\"Face\", face_image)\n",
    "\n",
    "                        # Compute the face encoding of the face\n",
    "                        face_encodings = face_recognition.face_encodings(face_image)\n",
    "                        print('face encoded!!!', face_encodings)\n",
    "                        # Continue with the rest of your code...\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 tv, 162.9ms\n",
      "Speed: 4.0ms preprocess, 162.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x0000015860C87FB0>, array([[[154, 156, 146],\n        [154, 155, 144],\n        [154, 152, 141],\n        ...,\n        [159, 164, 171],\n        [160, 165, 173],\n        [159, 164, 172]],\n\n       [[156, 156, 145],\n        [163, 162, 150],\n        [146, 144, 133],\n        ...,\n        [160, 163, 169],\n        [162, 165, 173],\n        [162, 165, 174]],\n\n       [[150, 150, 141],\n        [156, 156, 146],\n        [145, 145, 136],\n        ...,\n        [159, 162, 168],\n        [168, 171, 178],\n        [174, 177, 183]],\n\n       ...,\n\n       [[ 63,  85,  99],\n        [ 59,  82,  98],\n        [ 56,  79,  98],\n        ...,\n        [ 86,  82,  60],\n        [ 87,  82,  60],\n        [ 86,  80,  58]],\n\n       [[ 60,  82,  98],\n        [ 59,  80,  99],\n        [ 57,  77,  98],\n        ...,\n        [ 82,  78,  55],\n        [ 84,  80,  58],\n        [ 84,  80,  58]],\n\n       [[ 62,  79,  96],\n        [ 60,  76,  96],\n        [ 58,  73,  97],\n        ...,\n        [ 77,  76,  51],\n        [ 79,  78,  52],\n        [ 86,  84,  56]]], dtype=uint8), rectangle(294,54,381,141)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\justa\\OneDrive\\Desktop\\Developer\\CS4245_cv_for_lectures\\playgroud_notebook.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m faces:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m rect \u001b[39min\u001b[39;00m faces:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         shape \u001b[39m=\u001b[39m face_rec_model\u001b[39m.\u001b[39;49mcompute_face_descriptor(person_image, rect)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         face_encoding \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(shape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justa/OneDrive/Desktop/Developer/CS4245_cv_for_lectures/playgroud_notebook.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFace encoding for face in person \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mface_encoding\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x0000015860C87FB0>, array([[[154, 156, 146],\n        [154, 155, 144],\n        [154, 152, 141],\n        ...,\n        [159, 164, 171],\n        [160, 165, 173],\n        [159, 164, 172]],\n\n       [[156, 156, 145],\n        [163, 162, 150],\n        [146, 144, 133],\n        ...,\n        [160, 163, 169],\n        [162, 165, 173],\n        [162, 165, 174]],\n\n       [[150, 150, 141],\n        [156, 156, 146],\n        [145, 145, 136],\n        ...,\n        [159, 162, 168],\n        [168, 171, 178],\n        [174, 177, 183]],\n\n       ...,\n\n       [[ 63,  85,  99],\n        [ 59,  82,  98],\n        [ 56,  79,  98],\n        ...,\n        [ 86,  82,  60],\n        [ 87,  82,  60],\n        [ 86,  80,  58]],\n\n       [[ 60,  82,  98],\n        [ 59,  80,  99],\n        [ 57,  77,  98],\n        ...,\n        [ 82,  78,  55],\n        [ 84,  80,  58],\n        [ 84,  80,  58]],\n\n       [[ 62,  79,  96],\n        [ 60,  76,  96],\n        [ 58,  73,  97],\n        ...,\n        [ 77,  76,  51],\n        [ 79,  78,  52],\n        [ 86,  84,  56]]], dtype=uint8), rectangle(294,54,381,141)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "# Load models\n",
    "model = YOLO('yolov8n.pt')\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "face_rec_model = dlib.face_recognition_model_v1(\"dlib_face_recognition_resnet_model_v1.dat\")\n",
    "\n",
    "# Start capturing the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:  # You want continuous detection\n",
    "    # Read the current frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply the model to the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Process the results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes)):\n",
    "            if boxes.cls[i].item() == 0:  # 'person'\n",
    "                # Get normalized bounding box coordinates\n",
    "                x1, y1, x2, y2 = boxes.xyxyn[i]\n",
    "\n",
    "                # Convert normalized coordinates to absolute coordinates\n",
    "                h, w = frame.shape[:2]\n",
    "                x1_abs, y1_abs, x2_abs, y2_abs = int(x1 * w), int(y1 * h), int(x2 * w), int(y2 * h)\n",
    "\n",
    "                # Extract the person's image from the frame\n",
    "                person_image = frame[y1_abs:y2_abs, x1_abs:x2_abs]\n",
    "\n",
    "                # Detect faces within the person\n",
    "                faces = face_detector(person_image)\n",
    "\n",
    "                # For each face, compute face encoding\n",
    "                if faces:\n",
    "                    for rect in faces:\n",
    "                        shape = face_rec_model.compute_face_descriptor(person_image, rect)\n",
    "                        face_encoding = np.array(shape)\n",
    "\n",
    "                    print(f\"Face encoding for face in person {i}: {face_encoding}\")\n",
    "\n",
    "                    # Exit the loop after processing the first person\n",
    "                    break\n",
    "\n",
    "    # Display the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "    cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
